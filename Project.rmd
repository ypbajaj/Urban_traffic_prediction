---
title: "Time_Series_Project"
author: "Yash Bajaj"
date: "4/19/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(astsa)
library(dplyr)
library(faraway)
library(lmtest)
library(MASS)

library(car)

library(Metrics)
library(leaps)
```


```{r}
df <- read.csv('C:/Users/hp/Documents/R/STAT 429/Project/Behavior of the urban traffic of the city of Sao Paulo in Brazil/Behavior of the urban traffic of the city of Sao Paulo in Brazil.csv',sep=';')
df
```

```{r}
df$Slowness.in.traffic....<-as.numeric(gsub(",", ".", gsub("\\.", "", df$Slowness.in.traffic....)))
```

```{r}
df
```

```{r}
ts.plot(diff(df$Slowness.in.traffic....))
```



Monday - Friday
```{r}
a1 <- length(df$Slowness.in.traffic....)
b1 <- c()
weekday <- c('Monday','Tuesday','Wednesday','Thursday','Friday')
for(i in 0:(a1/27 -1)){
      b1[(27*i+1):(27*i+28)] <- weekday[((i%%length(weekday))+1)]
  }
df$weekday <- b1[1:length(df$Slowness.in.traffic....)]
```

```{r}
df
```

```{r}
data <- df
data$weekday <- as.factor(data$weekday)
data$Hour..Coded. <- as.factor(data$Hour..Coded.)
data$Time <- 1:135
```


```{r}
data
```

```{r}
summary(lm(Slowness.in.traffic....~Time,data=data))
```

```{r}
plot(data$Time,data$Slowness.in.traffic....)
```

```{r}
linear <- lm(Slowness.in.traffic....~.,data=data)
summary(linear)
```
```{r}
linear1 <- lm(Slowness.in.traffic....~.-Time,data=data)

x = model.matrix(linear1)[,-1] 
x = x - matrix(apply(x,2, mean), dim(x)[1],dim(x)[2], byrow=TRUE)
x = x / matrix(apply(x, 2, sd), dim(x)[1],dim(x)[2], byrow=TRUE)
#Extracting the eigen-values:
e = eigen(t(x) %*% x) 
sqrt(max(e$val)/min(e$val))
```

```{r}
vif(linear1)
```

#Leverage points
```{r}
n=dim(data)[1]; # sample size
p=19; # predictors plus intercept

# Compute Leverages
lev=influence(linear1)$hat

# Determine which exceed the 2p/n threshold
newlev = lev[lev>2*p/n]
length(newlev)
```

```{r}
length(newlev)/n
```

```{r}
# Prepare a half-normal plot 
halfnorm(lev, ylab="Leverages")
```

#Checking presence of outliers-
```{r}
jackknife=rstudent(linear1)
n = dim(data)[1]
p = 19
x=qt(.05/(2*n), n-p-1) #Significance level adjusted with Bonferroni's Correction
x
```
```{r}
sort(abs(jackknife),decreasing =T)[1:10]
```
#Checking Highly influential points -
```{r}
cook = cooks.distance(linear1)
# Extract max Cook's Distance
max(cook,na.rm=TRUE)
```

```{r}
halfnorm(cook, 6, labs=as.character(1:length(cook)), ylab="Cook's distances")
```

Since cook's distance is less than 1, we don't remove any data points.

#Checking for normality assumptions -
```{r}
plot(linear1,which=1)
```

Since there is no trend in our residual vs fitted values plot, we see
that normality assumptions hold true.

#Breush Pagan Test-
```{r}
bptest(linear1)
```
Since p-value of the test is greater than our significance level of 0.05 , we fail to reject H0 and conclude that the error variance is constant.

#Checking the normality of errors assumption-
Letâ€™s check if the errors in our model are normally distributed around a mean of zero. For that, we need to look at the histogram of the residuals as well as the Q-Q plot .
```{r}
hist(linear1$residuals, breaks = 10)
```

```{r}
plot(linear1,which = 2)
```

#Shapiro-Wilks test-
```{r}
shapiro.test(linear1$residuals)
```
#Box-Cox -
```{r}
box <- boxcox(Slowness.in.traffic....~.,data=data)
(lambda <- bc$x[which.max(bc$y)])
```

```{r}
tran_model <- lm(((Slowness.in.traffic....^lambda-1)/lambda) ~ .,data=data)
```

```{r}
shapiro.test(tran_model$residuals)
```

#Checking the linearity assumption-
```{r}
avPlots(linear)
```

#Train Test Split-
```{r}
drop <- c('Time')
data.new = data[,!(names(data) %in% drop)]
names(data.new)
```
```{r}
train_index = data.new["weekday"] == 'Friday'
train = data.new[!train_index,]
test = data.new[train_index,]

dim(train)
```

```{r}
step_linear <- step(linear,direction='both')
```

```{r}
summary(step_linear)
```

```{r}
summary(lm(Slowness.in.traffic.... ~ Hour..Coded. + Accident.victim + Occurrence.involving.freight + 
    Lack.of.electricity + Point.of.flooding + Defect.in.the.network.of.trolleybuses + 
    Tree.on.the.road + Intermittent.Semaphore + weekday,data=data))
```
```{r}
Hour..Coded.
lack
point.of
tree
weekday
```

```{r}
fm <- lm(Slowness.in.traffic.... ~ Hour..Coded. +  Lack.of.electricity + Point.of.flooding +  Tree.on.the.road +  weekday,data=data)
summary(fm)
```
```{r}
library(car)
scatterplot(Slowness.in.traffic....~.,data=data)
```

```{r}
dime <- dim(data)[2]
for (i in 1:dime){
  plot(data[,i],data$Slowness.in.traffic....,main=paste(colnames(data)[i]))
}
```

```{r}
plot(fm)
```

```{r}
acf2(fm$residuals)
```





```{r}
str(df1)

mod<-lm(sales~.,data=df1)
summary(mod)
```



```{r}
acf2(fm$residuals)
```

```{r}
auto.arima(fm$residuals)
```
```{r}
#Leaps
library(leaps)
leap <- regsubsets(Slowness.in.traffic....~.,data=data,nvmax=19)
var <- summary(leap)
names(var)
```
```{r}
coef(leap, which.min(var$cp))
```

```{r}
coef(leap, which.min(var$adjr2))
```
```{r}
coef(leap, which.min(var$bic))
```

```{r}
par(mfrow = c(2, 2))
plot(var$rss, xlab = "Number of Variables", ylab = "RSS", type = "b")

plot(var$adjr2, xlab = "Number of Variables", ylab = "Adjusted RSq", type = "b")
best_adj_r2 = which.max(var$adjr2)
points(best_adj_r2, var$adjr2[best_adj_r2],
       col = "red",cex = 2, pch = 20)

plot(var$cp, xlab = "Number of Variables", ylab = "Cp", type = 'b')
best_cp = which.min(var$cp)
points(best_cp, var$cp[best_cp], 
       col = "red", cex = 2, pch = 20)

plot(var$bic, xlab = "Number of Variables", ylab = "BIC", type = 'b')
best_bic = which.min(var$bic)
points(best_bic, var$bic[best_bic], 
       col = "red", cex = 2, pch = 20)
```


```{r}

design <- data[,c('Hour..Coded.','Lack.of.electricity','Point.of.flooding','Tree.on.the.road','weekday')]
design$weekday <- ifelse(design$weekday == 'Monday',1,ifelse(design$weekday == 'Tuesday',2,ifelse(design$weekday == 'Wednesday',3,ifelse(design$weekday == 'Thursday',4,5))))
sarima(fm,1,0,0,xreg = design)
```

#```{r}
<!-- # df1 <- read.csv('C:/Users/hp/Documents/R/STAT 429/Project/train.csv') -->
<!-- # df1 -->
<!-- # df2<-read.csv('C:/Users/hp/Documents/R/STAT 429/Project/test.csv') -->

<!-- df2$date <- as.Date(df2$date) -->
<!-- df2$date<-as.POSIXct(df2$date) -->

<!-- df2$month<-factor(format(df2$date,"%m")) -->
<!-- df2$year<-as.numeric(format(df2$date,"%Y")) -->
<!-- df2$day<-factor(format(df2$date,"%d")) -->
<!-- df2$wday <- factor(weekdays(as.Date(df2$date))) -->
<!-- df2$store<-factor(df2$store) -->

<!-- df2$item<-factor(df2$item) -->
<!-- df2<-df2[,!colnames(df2) %in% c('date','id')] -->
<!-- names(df2) -->
<!-- y_pred<-predict(mod,newdata = df2) -->

# ``````{r}
# trans_df<-function(df1){
# df1$date <- as.Date(df1$date)
# df1$date<-as.POSIXct(df1$date)
# 
# df1$month<-factor(format(df1$date,"%m"))
# df1$year<-as.numeric(format(df1$date,"%Y"))
# df1$day<-factor(format(df1$date,"%d"))
# df1$wday <- factor(weekdays(as.Date(df1$date)))
# df1$store<-factor(df1$store)
# 
# df1$item<-factor(df1$item)
# df1<-df1[,!colnames(df1) %in% c('date','id')]
# }
# 
# ```
# ```{r}
# df1<-trans_df(df1)

```